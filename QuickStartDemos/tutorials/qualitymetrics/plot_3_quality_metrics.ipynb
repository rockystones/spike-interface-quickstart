{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Quality Metrics Tutorial\n\nAfter spike sorting, you might want to validate the 'goodness' of the sorted units. This can be done using the\n:code:`qualitymetrics` submodule, which computes several quality metrics of the sorted units.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import spikeinterface.core as si\nimport spikeinterface.extractors as se\nfrom spikeinterface.postprocessing import compute_principal_components\nfrom spikeinterface.qualitymetrics import (\n    compute_snrs,\n    compute_firing_rates,\n    compute_isi_violations,\n    calculate_pc_metrics,\n    compute_quality_metrics,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's download a simulated dataset\nfrom the repo 'https://gin.g-node.org/NeuralEnsemble/ephy_testing_data'\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "local_path = si.download_dataset(remote_path=\"mearec/mearec_test_10s.h5\")\nrecording, sorting = se.read_mearec(local_path)\nprint(recording)\nprint(sorting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create SortingAnalyzer\n\nFor quality metrics we need first to create a :code:`SortingAnalyzer`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analyzer = si.create_sorting_analyzer(sorting=sorting, recording=recording, format=\"memory\")\nprint(analyzer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Depending on which metrics we want to compute we will need first to compute\nsome necessary extensions. (if not computed an error message will be raised)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analyzer.compute(\"random_spikes\", method=\"uniform\", max_spikes_per_unit=600, seed=2205)\nanalyzer.compute(\"waveforms\", ms_before=1.3, ms_after=2.6, n_jobs=2)\nanalyzer.compute(\"templates\", operators=[\"average\", \"median\", \"std\"])\nanalyzer.compute(\"noise_levels\")\n\nprint(analyzer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :code:`spikeinterface.qualitymetrics` submodule has a set of functions that allow users to compute\nmetrics in a compact and easy way. To compute a single metric, one can simply run one of the\nquality metric functions as shown below. Each function has a variety of adjustable parameters that can be tuned.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "firing_rates = compute_firing_rates(analyzer)\nprint(firing_rates)\nisi_violation_ratio, isi_violations_count = compute_isi_violations(analyzer)\nprint(isi_violation_ratio)\nsnrs = compute_snrs(analyzer)\nprint(snrs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To compute more than one metric at once, we can use the :code:`compute_quality_metrics` function and indicate\nwhich metrics we want to compute. This will return a pandas dataframe:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metrics = compute_quality_metrics(analyzer, metric_names=[\"firing_rate\", \"snr\", \"amplitude_cutoff\"])\nprint(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some metrics are based on the principal component scores, so the exwtension\nneed to be computed before. For instance:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analyzer.compute(\"principal_components\", n_components=3, mode=\"by_channel_global\", whiten=True)\n\nmetrics = compute_quality_metrics(\n    analyzer,\n    metric_names=[\n        \"isolation_distance\",\n        \"d_prime\",\n    ],\n)\nprint(metrics)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}