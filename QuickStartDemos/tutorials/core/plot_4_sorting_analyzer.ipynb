{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# SortingAnalyzer\n\nSpikeInterface provides an object to gather a Recording and a Sorting to perform various\nanalyses and visualizations of the sorting : :py:class:`~spikeinterface.core.SortingAnalyzer`.\n\nThis :py:class:`~spikeinterface.core.SortingAnalyzer` class:\n\n  * is the first step for all post post processing, quality metrics, and visualization.\n  * gathers a recording and a sorting\n  * can be sparse or dense : (i.e. whether all channel are used for all units or not).\n  * handle a list of \"extensions\"\n  * \"core extensions\" are the ones to extract some waveforms to compute templates:\n    * \"random_spikes\" : select randomly a subset of spikes per unit\n    * \"waveforms\" : extract waveforms per unit\n    * \"templates\": compute templates using average or median\n    * \"noise_levels\" : compute noise levels from traces (useful to get the snr of units)\n  * can be in memory or persistent to disk (2 formats binary/npy or zarr)\n\nMore extensions are available in `spikeinterface.postprocessing` like \"principal_components\", \"spike_amplitudes\",\n\"unit_lcations\", ...\n\n\nHere is the how!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nfrom spikeinterface import download_dataset\nfrom spikeinterface import create_sorting_analyzer, load_sorting_analyzer\nimport spikeinterface.extractors as se"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First let's use the repo https://gin.g-node.org/NeuralEnsemble/ephy_testing_data\nto download a MEArec dataset. It is a simulated dataset that contains \"ground truth\"\nsorting information:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "repo = \"https://gin.g-node.org/NeuralEnsemble/ephy_testing_data\"\nremote_path = \"mearec/mearec_test_10s.h5\"\nlocal_path = download_dataset(repo=repo, remote_path=remote_path, local_folder=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now instantiate the recording and sorting objects:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "recording = se.MEArecRecordingExtractor(local_path)\nprint(recording)\nsorting = se.MEArecSortingExtractor(local_path)\nprint(sorting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The MEArec dataset already contains a probe object that you can retrieve\nand plot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "probe = recording.get_probe()\nprint(probe)\nfrom probeinterface.plotting import plot_probe\n\nplot_probe(probe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A :py:class:`~spikeinterface.core.SortingAnalyzer` object can be created with the\n:py:func:`~spikeinterface.core.create_sorting_analyzer` function (this defaults to a sparse\nrepresentation of the waveforms)\nHere the format is \"memory\".\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analyzer = create_sorting_analyzer(sorting=sorting, recording=recording, format=\"memory\")\nprint(analyzer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A :py:class:`~spikeinterface.core.SortingAnalyzer` object can be persistant to disk\nwhen using format=\"binary_folder\" or format=\"zarr\". Read more about saving\nsorting analyzers in the [core documentation](https://spikeinterface.readthedocs.io/en/latest/modules/core.html#sortinganalyzer).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "folder = \"analyzer_folder\"\nanalyzer = create_sorting_analyzer(sorting=sorting,\n                                   recording=recording,\n                                   format=\"binary_folder\",\n                                   return_scaled=True, # this is the default to attempt to return scaled\n                                   folder=folder\n                                   )\nprint(analyzer)\n\n# then it can be loaded back\nanalyzer = load_sorting_analyzer(folder)\nprint(analyzer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No extensions are computed yet.\nLets compute the most basic ones : select some random spikes per units,\nextract waveforms (sparse in this example) and compute templates.\nYou can see that printing the object indicates which extension are already computed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analyzer.compute(\n    \"random_spikes\",\n    method=\"uniform\",\n    max_spikes_per_unit=500,\n)\nanalyzer.compute(\"waveforms\", ms_before=1.0, ms_after=2.0)\nanalyzer.compute(\"templates\", operators=[\"average\", \"median\", \"std\"])\nprint(analyzer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To speed up computation, some steps like \"waveforms\" can also be extracted\nusing parallel processing (recommended!). Like this\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analyzer.compute(\n    \"waveforms\", ms_before=1.0, ms_after=2.0, n_jobs=8, chunk_duration=\"1s\", progress_bar=True\n)\n\n# which is equivalent to this:\njob_kwargs = dict(n_jobs=8, chunk_duration=\"1s\", progress_bar=True)\nanalyzer.compute(\"waveforms\", ms_before=1.0, ms_after=2.0, **job_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because certain extensions rely on others (e.g. we need waveforms to calculate\nthe templates) if we recompute an extension it will delete any children extensions.\nSince we just recalculated \"waveforms\" when we print our analyzer we will see\nthat it no longer has templates: (Read more about parent/child dependency in the\n[core](https://spikeinterface.readthedocs.io/en/latest/modules/core.html#sortinganalyzer)\nand [postprocessing](https://spikeinterface.readthedocs.io/en/latest/modules/postprocessing.html) documentation.)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(analyzer)\n\n# so let's get our templates back.\nanalyzer.compute(\"templates\", operators=[\"average\", \"median\", \"std\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that there are a few different ways to calculate extensions. You might find it easier\nto keep the calculation parameters in a dictionary. Let's recalculate everything using this notation\n(note that extensions will be recomputed when we call compute even if the parameters are the same):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "job_kwargs = dict(n_jobs=8, chunk_duration=\"1s\", progress_bar=True)\ncompute_dict = {\n    'random_spikes': {'method': 'uniform', 'max_spikes_per_unit': 500},\n    'waveforms': {'ms_before': 1.0, 'ms_after': 2.0},\n    'templates': {'operators': [\"average\", \"median\", \"std\"]}\n}\nanalyzer.compute(compute_dict, **job_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each extension can retrieve some data\nFor instance the \"waveforms\" extension can retrieve waveforms per units\nwhich is a numpy array of shape (num_spikes, num_sample, num_channel):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ext_wf = analyzer.get_extension(\"waveforms\")\nfor unit_id in analyzer.unit_ids:\n    wfs = ext_wf.get_waveforms_one_unit(unit_id)\n    print(unit_id, \":\", wfs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Same for the \"templates\" extension. Here we can get all templates at once\nwith shape (num_units, num_sample, num_channel):\nFor this extension, we can get the template for all units either using the median\nor the average\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ext_templates = analyzer.get_extension(\"templates\")\n\nav_templates = ext_templates.get_data(operator=\"average\")\nprint(av_templates.shape)\n\nmedian_templates = ext_templates.get_data(operator=\"median\")\nprint(median_templates.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This can be plotted easily.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for unit_index, unit_id in enumerate(analyzer.unit_ids[:3]):\n    fig, ax = plt.subplots()\n    template = av_templates[unit_index]\n    ax.plot(template)\n    ax.set_title(f\"{unit_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The SortingAnalyzer can be saved to another format using save_as()\nSo the computation can be done with format=\"memory\" and then saved to disk\nin the zarr format by using save_as(). This will also save all the extensions\nin their current state.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analyzer.save_as(folder=\"analyzer.zarr\", format=\"zarr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The SortingAnalyzer also offers select_units() method which allows exporting\nonly some relevant units for instance to a new SortingAnalyzer instance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analyzer_some_units = analyzer.select_units(\n    unit_ids=analyzer.unit_ids[:5], format=\"binary_folder\", folder=\"analyzer_some_units\"\n)\nprint(analyzer_some_units)\n\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}