{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Model-based curation tutorial\n\nSorters are not perfect. They output excellent units, as well as noisy ones, and ones that\nshould be split or merged. Hence one should curate the generated units. Historically, this\nhas been done using laborious manual curation. An alternative is to use automated methods\nbased on metrics which quantify features of the units. In spikeinterface these are the\nquality metrics and the template metrics. A simple approach is to use thresholding:\nonly accept units whose metrics pass a certain quality threshold. Another approach is to\ntake one (or more) manually labelled sortings, whose metrics have been computed, and train\na machine learning model to predict labels.\n\nThis notebook provides a step-by-step guide on how to take a machine learning model that\nsomeone else has trained and use it to curate your own spike sorted output. SpikeInterface\nalso provides the tools to train your own model,\n[which you can learn about here](https://spikeinterface.readthedocs.io/en/latest/tutorials/curation/plot_2_train_a_model.html).\n\nWe'll download a toy model and use it to label our sorted data. We start by importing some packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport spikeinterface.core as si\nimport spikeinterface.curation as sc\nimport spikeinterface.widgets as sw\n\n# note: you can use more cores using e.g.\n# si.set_global_jobs_kwargs(n_jobs = 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download a pretrained model\n\nLet's download a pretrained model from [Hugging Face](https://huggingface.co/) (HF),\na model sharing platform focused on AI and ML models and datasets. The\n``load_model`` function allows us to download directly from HF, or use a model in a local\nfolder. The function downloads the model and saves it in a temporary folder and returns a\nmodel and some metadata about the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model, model_info = sc.load_model(\n    repo_id = \"SpikeInterface/toy_tetrode_model\",\n    trusted = ['numpy.dtype']\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model was trained on artifically generated tetrode data. There are also models trained\non real data, like the one discussed [below](#A-model-trained-on-real-Neuropixels-data).\nEach model object has a nice html representation, which will appear if you're using a Jupyter notebook.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tells us more information about the model. The one we've just downloaded was trained used\na ``RandomForestClassifier```. You can also discover this information by running\n``model.get_params()``. The model object (an [sklearn Pipeline](https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html)) also contains information\nabout which metrics were used to compute the model. We can access it from the model (or from the model_info)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(model.feature_names_in_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hence, to use this model we need to create a ``sorting_analyzer`` with all these metrics computed.\nWe'll do this by generating a recording and sorting, creating a sorting analyzer and computing a\nbunch of extensions. Follow these links for more info on [recordings](https://spikeinterface.readthedocs.io/en/latest/modules/extractors.html), [sortings](https://spikeinterface.readthedocs.io/en/latest/modules/sorters.html), [sorting analyzers](https://spikeinterface.readthedocs.io/en/latest/tutorials/core/plot_4_sorting_analyzer.html#sphx-glr-tutorials-core-plot-4-sorting-analyzer-py)\nand [extensions](https://spikeinterface.readthedocs.io/en/latest/modules/postprocessing.html).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "recording, sorting = si.generate_ground_truth_recording(num_channels=4, seed=4, num_units=10)\nsorting_analyzer = si.create_sorting_analyzer(sorting=sorting, recording=recording)\nsorting_analyzer.compute(['noise_levels','random_spikes','waveforms','templates','spike_locations','spike_amplitudes','correlograms','principal_components','quality_metrics','template_metrics'])\nsorting_analyzer.compute('template_metrics', include_multi_channel_metrics=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This sorting_analyzer now contains the required quality metrics and template metrics.\nWe can check that this is true by accessing the extension data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "all_metric_names = list(sorting_analyzer.get_extension('quality_metrics').get_data().keys()) + list(sorting_analyzer.get_extension('template_metrics').get_data().keys())\nprint(set(model.feature_names_in_).issubset(set(all_metric_names)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! We can now use the model to predict labels. Here, we pass the HF repo id directly\nto the ``auto_label_units`` function. This returns a dictionary containing a label and\na confidence for each unit contained in the ``sorting_analyzer``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "labels = sc.auto_label_units(\n    sorting_analyzer = sorting_analyzer,\n    repo_id = \"SpikeInterface/toy_tetrode_model\",\n    trusted = ['numpy.dtype']\n)\n\nprint(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model has labelled one unit as bad. Let's look at that one, and also the 'good' unit\nwith the highest confidence of being 'good'.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sw.plot_unit_templates(sorting_analyzer, unit_ids=['7','9'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nice! Unit 9 looks more like an expected action potential waveform while unit 7 doesn't,\nand it seems reasonable that unit 7 is labelled as `bad`. However, for certain experiments\nor brain areas, unit 7 might be a great small-amplitude unit. This example highlights that\nyou should be careful applying models trained on one dataset to your own dataset. You can\nexplore the currently available models on the [spikeinterface hugging face hub](https://huggingface.co/SpikeInterface)\npage, or [train your own one](https://spikeinterface.readthedocs.io/en/latest/tutorials/curation/plot_2_train_a_model.html).\n\n## Assess the model performance\n\nTo assess the performance of the model relative to labels assigned by a human creator, we can load or generate some\n\"human labels\", and plot a confusion matrix of predicted vs human labels for all clusters. Here\nwe'll be a conservative human, who has labelled several units with small amplitudes as 'bad'.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "human_labels = ['bad', 'good', 'good', 'bad', 'good', 'bad', 'good', 'bad', 'good', 'good']\n\n# Note: if you labelled using phy, you can load the labels using:\n# human_labels = sorting_analyzer.sorting.get_property('quality')\n# We need to load in the `label_conversion` dictionary, which converts integers such\n# as '0' and '1' to readable labels such as 'good' and 'bad'. This is stored as\n# in `model_info`, which we loaded earlier.\n\nfrom sklearn.metrics import confusion_matrix, balanced_accuracy_score\n\nlabel_conversion = model_info['label_conversion']\npredictions = labels['prediction']\n\nconf_matrix = confusion_matrix(human_labels, predictions)\n\n# Calculate balanced accuracy for the confusion matrix\nbalanced_accuracy = balanced_accuracy_score(human_labels, predictions)\n\nplt.imshow(conf_matrix)\nfor (index, value) in np.ndenumerate(conf_matrix):\n    plt.annotate( str(value), xy=index, color=\"white\", fontsize=\"15\")\nplt.xlabel('Predicted Label')\nplt.ylabel('Human Label')\nplt.xticks(ticks = [0, 1], labels = list(label_conversion.values()))\nplt.yticks(ticks = [0, 1], labels = list(label_conversion.values()))\nplt.title('Predicted vs Human Label')\nplt.suptitle(f\"Balanced Accuracy: {balanced_accuracy}\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, there are several false positives (if we consider the human labels to be \"the truth\").\n\nNext, we can also see how the model's confidence relates to the probability that the model\nlabel matches the human label.\n\nThis could be used to help decide which units should be auto-curated and which need further\nmanual creation. For example, we might accept any unit as 'good' that the model predicts\nas 'good' with confidence over a threshold, say 80%. If the confidence is lower we might decide to take a\nlook at this unit manually. Below, we will create a plot that shows how the agreement\nbetween human and model labels changes as we increase the confidence threshold. We see that\nthe agreement increases as the confidence does. So the model gets more accurate with a\nhigher confidence threshold, as expceted.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def calculate_moving_avg(label_df, confidence_label, window_size):\n\n    label_df[f'{confidence_label}_decile'] = pd.cut(label_df[confidence_label], 10, labels=False, duplicates='drop')\n    # Group by decile and calculate the proportion of correct labels (agreement)\n    p_label_grouped = label_df.groupby(f'{confidence_label}_decile')['model_x_human_agreement'].mean()\n    # Convert decile to range 0-1\n    p_label_grouped.index = p_label_grouped.index / 10\n    # Sort the DataFrame by confidence scores\n    label_df_sorted = label_df.sort_values(by=confidence_label)\n\n    p_label_moving_avg = label_df_sorted['model_x_human_agreement'].rolling(window=window_size).mean()\n\n    return label_df_sorted[confidence_label], p_label_moving_avg\n\nconfidences = labels['probability']\n\n# Make dataframe of human label, model label, and confidence\nlabel_df = pd.DataFrame(data = {\n    'human_label': human_labels,\n    'decoder_label': predictions,\n    'confidence': confidences},\n    index = sorting_analyzer.sorting.get_unit_ids())\n\n# Calculate the proportion of agreed labels by confidence decile\nlabel_df['model_x_human_agreement'] = label_df['human_label'] == label_df['decoder_label']\n\np_agreement_sorted, p_agreement_moving_avg = calculate_moving_avg(label_df, 'confidence', 3)\n\n# Plot the moving average of agreement\nplt.figure(figsize=(6, 6))\nplt.plot(p_agreement_sorted, p_agreement_moving_avg, label = 'Moving Average')\nplt.axhline(y=1/len(np.unique(predictions)), color='black', linestyle='--', label='Chance')\nplt.xlabel('Confidence'); #plt.xlim(0.5, 1)\nplt.ylabel('Proportion Agreement with Human Label'); plt.ylim(0, 1)\nplt.title('Agreement vs Confidence (Moving Average)')\nplt.legend(); plt.grid(True); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, you might decide to only trust labels which had confidence over above 0.88,\nand manually labels the ones the model isn't so confident about.\n\n## A model trained on real Neuropixels data\n\nAbove, we used a toy model trained on generated data. There are also models on HuggingFace\ntrained on real data.\n\nFor example, the following classifiers are trained on Neuropixels data from 11 mice recorded in\nV1,SC and ALM: https://huggingface.co/SpikeInterface/UnitRefine_noise_neural_classifier/ and\nhttps://huggingface.co/SpikeInterface/UnitRefine_sua_mua_classifier/. One will classify units into\n`noise` or `not-noise` and the other will classify the `not-noise` units into single\nunit activity (sua) units and multi-unit activity (mua) units.\n\nThere is more information about the model on the model's HuggingFace page. Take a look!\nThe idea here is to first apply the noise/not-noise classifier, then the sua/mua one.\nWe can do so as follows:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Apply the noise/not-noise model\nnoise_neuron_labels = sc.auto_label_units(\n    sorting_analyzer=sorting_analyzer,\n    repo_id=\"SpikeInterface/UnitRefine_noise_neural_classifier\",\n    trust_model=True,\n)\n\nnoise_units = noise_neuron_labels[noise_neuron_labels['prediction']=='noise']\nanalyzer_neural = sorting_analyzer.remove_units(noise_units.index)\n\n# Apply the sua/mua model\nsua_mua_labels = sc.auto_label_units(\n    sorting_analyzer=analyzer_neural,\n    repo_id=\"SpikeInterface/UnitRefine_sua_mua_classifier\",\n    trust_model=True,\n)\n\nall_labels = pd.concat([sua_mua_labels, noise_units]).sort_index()\nprint(all_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you run this without the ``trust_model=True`` parameter, you will receive an error:\n\n```\nUntrustedTypesFoundException: Untrusted types found in the file: ['sklearn.metrics._classification.balanced_accuracy_score', 'sklearn.metrics._scorer._Scorer', 'sklearn.model_selection._search_successive_halving.HalvingGridSearchCV', 'sklearn.model_selection._split.StratifiedKFold']\n```\nThis is a security warning, which can be overcome by passing the trusted types list\n``trusted = ['sklearn.metrics._classification.balanced_accuracy_score', 'sklearn.metrics._scorer._Scorer', 'sklearn.model_selection._search_successive_halving.HalvingGridSearchCV', 'sklearn.model_selection._split.StratifiedKFold']``\nor by passing the ``trust_model=True``` keyword.\n\n.. dropdown:: More about security\n\n  Sharing models, with are Python objects, is complicated.\n  We have chosen to use the [skops format](https://skops.readthedocs.io/en/stable/), instead\n  of the common but insecure ``.pkl`` format (read about ``pickle`` security issues\n  [here](https://lwn.net/Articles/964392/)). While unpacking the ``.skops`` file, each function\n  is checked. Ideally, skops should recognise all `sklearn`, `numpy` and `scipy` functions and\n  allow the object to be loaded if it only contains these (and no unkown malicious code). But\n  when ``skops`` it's not sure, it raises an error. Here, it doesn't recognise\n  ``['sklearn.metrics._classification.balanced_accuracy_score', 'sklearn.metrics._scorer._Scorer',\n  'sklearn.model_selection._search_successive_halving.HalvingGridSearchCV',\n  'sklearn.model_selection._split.StratifiedKFold']``. Taking a look, these are all functions\n  from `sklearn`, and we can happily add them to the ``trusted`` functions to load.\n\n   In general, you should be cautious when downloading ``.skops`` files and ``.pkl`` files from repos,\n   especially from unknown sources.\n\n## Directly applying a sklearn Pipeline\n\nInstead of using ``HuggingFace`` and ``skops``, someone might have given you a model\nin differet way: perhaps by e-mail or a download. If you have the model in a\nfolder, you can apply it in a very similar way:\n\n```\nlabels = sc.auto_label_units(\n    sorting_analyzer = sorting_analyzer,\n    model_folder = \"path/to/model/folder\",\n)\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using this, you lose the advantages of the model metadata: the quality metric parameters\nare not checked and the labels are not converted their original human readable names (like\n'good' and 'bad'). Hence we advise using the methods discussed above, when possible.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}