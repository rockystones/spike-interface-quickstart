{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training a model for automated curation\n\nIf the pretrained models do not give satisfactory performance on your data, it is easy to train your own classifier using SpikeInterface.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Generate and label data\n\nFirst we will import our dependencies\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport spikeinterface.core as si\nimport spikeinterface.curation as sc\nimport spikeinterface.widgets as sw\n\n# Note, you can set the number of cores you use using e.g.\n# si.set_global_job_kwargs(n_jobs = 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this tutorial, we will use simulated data to create ``recording`` and ``sorting`` objects. We'll\ncreate two sorting objects: :code:`sorting_1` is coupled to the real recording, so the spike times of the sorter will\nperfectly match the spikes in the recording. Hence this will contain good units. However, we've\nuncoupled :code:`sorting_2` to the recording and the spike times will not be matched with the spikes in the recording.\nHence these units will mostly be random noise. We'll combine the \"good\" and \"noise\" sortings into one sorting\nobject using :code:`si.aggregate_units`.\n\n(When making your own model, you should\n[load your own recording](https://spikeinterface.readthedocs.io/en/latest/modules/extractors.html)\nand [do a sorting](https://spikeinterface.readthedocs.io/en/latest/modules/sorters.html) on your data.)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "recording, sorting_1 = si.generate_ground_truth_recording(num_channels=4, seed=1, num_units=5)\n_, sorting_2 =si.generate_ground_truth_recording(num_channels=4, seed=2, num_units=5)\n\nboth_sortings = si.aggregate_units([sorting_1, sorting_2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To do some visualisation and postprocessing, we need to create a sorting analyzer, and\ncompute some extensions:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analyzer = si.create_sorting_analyzer(sorting = both_sortings, recording=recording)\nanalyzer.compute(['noise_levels','random_spikes','waveforms','templates'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can plot the templates for the first and fifth units. The first (unit id 0) belongs to\n:code:`sorting_1` so should look like a real unit; the sixth (unit id 5) belongs to :code:`sorting_2`\nso should look like noise.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sw.plot_unit_templates(analyzer, unit_ids=[\"0\", \"5\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is as expected: great! (Find out more about plotting [using widgets](https://spikeinterface.readthedocs.io/en/latest/modules/widgets.html).)\nWe've set up our system so that the first five units are 'good' and the next five are 'bad'.\nSo we can make a list of labels which contain this information. For real data, you could\nuse a manual curation tool to make your own list.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "labels = ['good', 'good', 'good', 'good', 'good', 'bad', 'bad', 'bad', 'bad', 'bad']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Train our model\n\nWe'll now train a model, based on our labelled data. The model will be trained using properties\nof the units, and then be applied to units from other sortings. The properties we use are the\n[quality metrics](https://spikeinterface.readthedocs.io/en/latest/modules/qualitymetrics.html)\nand [template metrics](https://spikeinterface.readthedocs.io/en/latest/modules/postprocessing.html#template-metrics).\nHence we need to compute these, using some ``sorting_analyzer``` extensions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analyzer.compute(['spike_locations','spike_amplitudes','correlograms','principal_components','quality_metrics','template_metrics'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have metrics and labels, we're ready to train the model using the\n``train_model``` function. The trainer will try several classifiers, imputation strategies and\nscaling techniques then save the most accurate. To save time in this tutorial,\nwe'll only try one classifier (Random Forest), imputation strategy (median) and scaling\ntechnique (standard scaler).\n\nWe will use a list of one analyzer here, so the model is trained on a single\nsession. In reality, we would usually train a model using multiple analyzers from an\nexperiment, which should make the model more robust. To do this, you can simply pass\na list of analyzers and a list of manually curated labels for each\nof these analyzers. Then the model would use all of these data as input.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer = sc.train_model(\n    mode = \"analyzers\", # You can supply a labelled csv file instead of an analyzer\n    labels = [labels],\n    analyzers = [analyzer],\n    folder = \"my_folder\", # Where to save the model and model_info.json file\n    metric_names = None, # Specify which metrics to use for training: by default uses those already calculted\n    imputation_strategies = [\"median\"], # Defaults to all\n    scaling_techniques = [\"standard_scaler\"], # Defaults to all\n    classifiers = None, # Default to Random Forest only. Other classifiers you can try [ \"AdaBoostClassifier\",\"GradientBoostingClassifier\",\"LogisticRegression\",\"MLPClassifier\"]\n    overwrite = True, # Whether or not to overwrite `folder` if it already exists. Default is False.\n    search_kwargs = {'cv': 3} # Parameters used during the model hyperparameter search\n)\n\nbest_model = trainer.best_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can pass many sklearn [classifiers](https://scikit-learn.org/1.5/api/sklearn.impute.html)\n[imputation strategies](https://scikit-learn.org/1.5/api/sklearn.impute.html) and\n[scalers](https://scikit-learn.org/1.5/api/sklearn.preprocessing.html), although the\ndocumentation is quite overwhelming. You can find the classifiers we've tried out\nusing the ``sc.get_default_classifier_search_spaces`` function.\n\nThe above code saves the model in ``model.skops``, some metadata in\n``model_info.json`` and the model accuracies in ``model_accuracies.csv``\nin the specified ``folder`` (in this case ``'my_folder'``).\n\n(``skops`` is a file format: you can think of it as a more-secure pkl file. [Read more](https://skops.readthedocs.io/en/stable/index.html).)\n\nThe ``model_accuracies.csv`` file contains the accuracy, precision and recall of the\ntested models. Let's take a look:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "accuracies = pd.read_csv(Path(\"my_folder\") / \"model_accuracies.csv\", index_col = 0)\naccuracies.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our model is perfect!! This is because the task was *very* easy. We had 10 units; where\nhalf were pure noise and half were not.\n\nThe model also contains some more information, such as which features are \"important\",\nas defined by sklearn (learn about feature importance of a\n[Random Forest Classifier](https://scikit-learn.org/1.5/auto_examples/ensemble/plot_forest_importances.html).)\nWe can plot these:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot feature importances\nimportances = best_model.named_steps['classifier'].feature_importances_\nindices = np.argsort(importances)[::-1]\n\n# The sklearn importances are not computed for inputs whose values are all `nan`.\n# Hence, we need to pick out the non-`nan` columns of our metrics\nfeatures = best_model.feature_names_in_\nn_features = best_model.n_features_in_\n\nmetrics = pd.concat([analyzer.get_extension('quality_metrics').get_data(), analyzer.get_extension('template_metrics').get_data()], axis=1)\nnon_null_metrics = ~(metrics.isnull().all()).values\n\nfeatures = features[non_null_metrics]\nn_features = len(features)\n\nplt.figure(figsize=(12, 7))\nplt.title(\"Feature Importances\")\nplt.bar(range(n_features), importances[indices], align=\"center\")\nplt.xticks(range(n_features), features[indices], rotation=90)\nplt.xlim([-1, n_features])\nplt.subplots_adjust(bottom=0.3)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Roughly, this means the model is using metrics such as \"nn_hit_rate\" and \"l_ratio\"\nbut is not using \"sync_spike_4\" and \"rp_contanimation\". This is a toy model, so don't\ntake these results seriously. But using this information, you could retrain another,\nsimpler model using a subset of the metrics, by passing, e.g.,\n``metric_names = ['nn_hit_rate', 'l_ratio',...]`` to the ``train_model`` function.\n\nNow that you have a model, you can [apply it to another sorting](https://spikeinterface.readthedocs.io/en/latest/tutorials/curation/plot_1_automated_curation.html)\nor [upload it to HuggingFaceHub](https://spikeinterface.readthedocs.io/en/latest/tutorials/curation/plot_3_upload_a_model.html).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}